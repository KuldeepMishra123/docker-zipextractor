## Overview

#### Problem Statement :
1) Unzip the tar file containing multiple files (json,pdf ) with size in GBs .
2) Json files are having complex schema type i.e. Struct type . 
3) Flatten out these Json and parse them into tabular format.
3) Store all parsed Jsons into DB Engine .
4) Write sample queries on these tables .  

#### Assumption :
* HDFS, Spark and Docker is installed in the execution environment.
* Tar file would be available in HDFS for distributed processing.   
* Tar file is the same file which is available in [yelp_dataset.tgz](https://www.yelp.com/dataset).
* Json file names are always same and are available in tar .

#### Prerequisites
The Project uses below api versions :

|API name|Version|
|---|---|
|spark-core_2.12|3.0.0|
|spark-sql_2.12|3.0.0|
|Apache Maven|3.5.4|
|Scala|2.12|
|Docker|19.03.13|
|postgresql|42.2.2|

#### To run the project in the Intellij
* Unzip the provided zip file
* In a terminal window, navigate to repo folder and run `docker-compose up` to build and start the containers postgres container.
* Import the project in IntelliJ and add the framework as Maven.
* clean and package the build .
* Copy the jar/jar_with_dependencies to hdfs environmrnt for spark-execution.
* run spark-submit giving the below parameters:-
  - --master : local or yarn or spark://Host1:Port1 or mesos://HOST:PORT
  - --deploy-mode : cluster or client (not required for local mode)
  - full path to application_jar
  - path to hdfs folder where yelp_dataset.tgz is present i.e. tarfolder/
  - (optional) path to hdfs folder where json files would be unzipped. If not provided then files would be unzipped to output folder where tar is present i.e. tarfolder/output/
* for ex:- `spark-submit --class "com.kulexample.MainRunner" --master local[*] target/kulexample-1.0-jar-with-dependencies.jar D:/PracticeProjects/kulexample_scripts/yelp_dataset/`
* In another shell/terminal/linux window run `docker exec -it kulexample-postgres psql -U docker -d kulexample`  
* Run postges sql queries on the container. for ex:- 
  - `\dt` - for showing all tables generated by spark job.
  - `\d tablename` - for showing schema of table.
  - `select count(*) from yelp_academic_dataset_checkin;`
  - `select "business_id", "name", "city", "address", "attributes_WiFi" , "stars", "review_count" , "attributes_DogsAllowed", "hours_Sunday" from yelp_academic_dataset_business as b where  "city" = 'Las Vegas' and "hours_Sunday" is not null and "attributes_DogsAllowed" = 'True' and "attributes_WiFi" is not null order by "stars" desc , "review_count" desc limit 10; `


#### Solution :
###### Unzip : 
* `InputStream` As the size of file is huge, Tar is read in using InputStream, so that we can unpack the files one-at-a-time, without downloading the entire file or jumping around, reading non-contiguous chunks of the file. 
* The first step is to strip off the .gz compression for which `org.apache.commons.compress` libraries are used.
* The stream needs to support `mark()`, or you get an IllegalArgumentException: Mark is not supported – hence wrapping it in a BufferedInputStream. The result is an InputStream whose bytes are equivalent to running `gunzip numbers.tar.gz` on the command line.
* It strips off any compression layer. For example, if you have a tar.xz file, it strips off the .xz compression instead. 
* `ArchiveInputStream` is a special type of InputStream that emits an EOF when it gets to the end of a file in the archive. Once it’s done, you call getNextEntry to reset the stream and start reading the next file. When getNextEntry returns null, you’re at the end of the archive.
* The `getNextEntry` method returns an ArchiveEntry, which includes the name and size of the original entry, and whether it’s a file or a directory.
* If you call `close()` on any of the individual streams, that gets passed down to the underlying stream. When you go to read the next entry, you get an `IOException: input buffer is closed`.
  We can’t stop downstream code from calling close(), but we can stop a close breaking the entire process. A `CloseShieldInputStream` prevents the underlying stream from being closed (even when you call close()) – this is just the sort of thing it’s designed for. 

###### Flattened Schema : 
*  Identify columns which are having Struct type. i.e complex datatype   
*  Recursively flat all columns until all Struct type columns are converted to `rootColumnName.leafColumnName` format.
*  combine schema of Non-Syruct type and flattened type columns using mutable array buffer.
*  Return flattened dataframe by selecting combined schema from original dataframedataframe  

###### Store into DB (Postgres) : 
*  Dockerised postgres sql engine is used as DB engine for ease of implementation for this PoC. 
*  `docker-compose up` will download postgres:9.6 image and start `kulexample-postgres` with provided environment details.
*  It will execute `db.sql` script available in `/repo_directory/kulexample/sql/db.sql` and create database named `kulexample` .
*  Parsed dataframes would be written to postgres kulexample db using `org.postgresql.Driver` .
*  If the table is not present then table would be created with passed config `.option("dbtable", tableName)`
*  If table is already populated then it will overwrite the table with `mode(SaveMode.Overwrite).` 

###### Sample queries : 
*  `\dt` will let us know available table in kulexample db. 
*  `\d yelp_academic_dataset_business` will show the flattened schema for all columns having struct datatype.
*  `select count(*) from yelp_academic_dataset_business ;` -- Aggregation
*  Find top 10 business_names/restaurant_names having highest star rating and review count in Las Vegas. Business should be opened on sunday and dogs should be allowed.
   - `select "business_id", "name", "city", "address", "attributes_WiFi" , "stars", "review_count" , "attributes_DogsAllowed", "hours_Sunday" from yelp_academic_dataset_business as b where  "city" = 'Las Vegas' and "hours_Sunday" is not null and "attributes_DogsAllowed" = 'True' and "attributes_WiFi" is not null order by "stars" desc , "review_count" desc limit 10; `
*  Find top 10 latest tips(quick comment) given by user to any business 
   - `select us.name user_name, busi.name business_name , tip.date tip_date, tip.text quick_tip from yelp_academic_dataset_tip tip inner join yelp_academic_dataset_user us on tip.user_id = us.user_id inner join yelp_academic_dataset_business busi on tip.business_id = busi.business_id order by tip_date limit 10;`
 
